{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4468d4eb-c6bb-4483-a153-e4a3011e42d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhd/anaconda3/envs/im2para/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.blip2_model import ImageCaptioning\n",
    "from models.grit_model import DenseCaptioning\n",
    "from models.gpt_model import ImageToText\n",
    "from models.region_semantic import RegionSemantic\n",
    "from mmengine import load, dump\n",
    "from utils.util import read_image_width_height, display_images_and_text, resize_long_edge\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    from mmcv import load, dump\n",
    "except:\n",
    "    from mmengine import load, dump\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--verbose', action='store_true', dest='verbose', default=False)\n",
    "parser.add_argument('--gpt_version', choices=['gpt-3.5-turbo', 'gpt4'], default='gpt-3.5-turbo')\n",
    "parser.add_argument('--image_caption', action='store_true', dest='image_caption', default=True, help='Set this flag to True if you want to use BLIP2 Image Caption')\n",
    "parser.add_argument('--dense_caption', action='store_true', dest='dense_caption', default=True, help='Set this flag to True if you want to use Dense Caption')\n",
    "parser.add_argument('--semantic_segment', action='store_true', dest='semantic_segment', default=True, help='Set this flag to True if you want to use semantic segmentation')\n",
    "parser.add_argument('--sam_arch', choices=['vit_b', 'vit_l', 'vit_h'], dest='sam_arch', default='vit_h', help='vit_b is the default model (fast but not accurate), vit_l and vit_h are larger models')\n",
    "parser.add_argument('--captioner_base_model', choices=['blip', 'blip2'], dest='captioner_base_model', default='blip2', help='blip2 requires 15G GPU memory, blip requires 6G GPU memory')\n",
    "parser.add_argument('--region_classify_model', choices=['ssa', 'edit_anything'], dest='region_classify_model', default='edit_anything', help='Select the region classification model: edit anything is ten times faster than ssa, but less accurate.')\n",
    "parser.add_argument('--device', default=None, help=\"if set, will override all other device arguments\")\n",
    "parser.add_argument('--image_caption_device', default='cuda:2', help='Select the device: cuda or cpu, gpu memory larger than 14G is recommended')\n",
    "parser.add_argument('--dense_caption_device', default='cuda:2', help='Select the device: cuda or cpu, < 6G GPU is not recommended>')\n",
    "parser.add_argument('--semantic_segment_device', default='cuda:2', help='Select the device: cuda or cpu, gpu memory larger than 14G is recommended. Make sue this model and image_caption model on same device.')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b974ce2-b916-4f5a-973f-47ced344e4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['OPENAI_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "535eca09-9022-4e55-88a6-ed82e7c7f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextTransformation:\n",
    "    def __init__(self, args):\n",
    "        # Load your big model here\n",
    "        self.args = args\n",
    "        self.verbose = args.verbose\n",
    "        if args.device is not None:\n",
    "            assert 'cuda' in args.device\n",
    "            args.image_caption_device = args.device\n",
    "            args.dense_caption_device = args.device\n",
    "            args.semantic_segment_device = args.device\n",
    "        self.init_models()\n",
    "    \n",
    "    def init_models(self):\n",
    "        openai_key = os.environ['OPENAI_KEY']\n",
    "        print(self.args)\n",
    "        print('\\033[1;34m' + \"Welcome to the Image2Paragraph toolbox...\".center(50, '-') + '\\033[0m')\n",
    "        print('\\033[1;33m' + \"Initializing models...\".center(50, '-') + '\\033[0m')\n",
    "        print('\\033[1;31m' + \"This is time-consuming, please wait...\".center(50, '-') + '\\033[0m')\n",
    "        self.image_caption_model = ImageCaptioning(device=self.args.image_caption_device, captioner_base_model=self.args.captioner_base_model, verbose=self.verbose)\n",
    "        self.dense_caption_model = DenseCaptioning(device=self.args.dense_caption_device, verbose=self.verbose)\n",
    "        self.gpt_model = ImageToText(openai_key)\n",
    "        self.region_semantic_model = RegionSemantic(\n",
    "            device=self.args.semantic_segment_device, \n",
    "            image_caption_model=self.image_caption_model, \n",
    "            region_classify_model=self.args.region_classify_model, \n",
    "            sam_arch=self.args.sam_arch, \n",
    "            verbose=self.verbose)\n",
    "        print('\\033[1;32m' + \"Model initialization finished!\".center(50, '-') + '\\033[0m')\n",
    "        \n",
    "    def boxstr_to_box(self, boxstr, old_shape, old_format='xyxy'):\n",
    "        boxstr = boxstr[1:-1]\n",
    "        box = [float(x) for x in boxstr.split(\", \")]\n",
    "        if old_format == 'xywh':\n",
    "            box = [box[0], box[1], box[0] + box[2], box[1] + box[3]]\n",
    "        # Since Dense caption & Region semantic model re-scale long edge to 384, we map it back here\n",
    "        ratio = max(old_shape) / 384\n",
    "        box = [int(ratio * x) for x in box]\n",
    "        return box\n",
    "        \n",
    "    def image_to_info(self, img_src):\n",
    "        width, height = read_image_width_height(img_src)\n",
    "        shape = (width, height)\n",
    "        # print(self.args)\n",
    "        if self.args.image_caption:\n",
    "            image_caption = self.image_caption_model.image_caption(img_src)\n",
    "        else:\n",
    "            image_caption = \" \"\n",
    "        if self.args.dense_caption:\n",
    "            dense_caption_str = self.dense_caption_model.image_dense_caption(img_src)\n",
    "            dense_caption = dense_caption_str.split(\"; \") \n",
    "            if \":\" not in dense_caption[-1]:\n",
    "                dense_caption = dense_caption[:-1]\n",
    "            dense_caption = [x.split(\": \") for x in dense_caption]\n",
    "            dense_caption = {x[0]: self.boxstr_to_box(x[1], shape) for x in dense_caption}\n",
    "        else:\n",
    "            dense_caption_str = \" \"\n",
    "            dense_caption = {}\n",
    "        if self.args.semantic_segment:\n",
    "            region_semantic_str = self.region_semantic_model.region_semantic(img_src)\n",
    "            region_semantic = region_semantic_str.split(\"; \") \n",
    "            if \":\" not in region_semantic[-1]:\n",
    "                region_semantic = region_semantic[:-1]\n",
    "            region_semantic = [x.split(\": \") for x in region_semantic]\n",
    "            region_semantic = {x[0]: self.boxstr_to_box(x[1], shape, 'xywh') for x in region_semantic}\n",
    "        else:\n",
    "            region_semantic_str = \" \"\n",
    "            region_semantic = {}\n",
    "        return dict(\n",
    "            shape=(width, height), caption=image_caption, dense_caption=dense_caption, region_semantic=region_semantic, \n",
    "            note=\"For dense caption and region_semantic, xyxy bounding box is returned\")\n",
    "    \n",
    "    def image_to_text(self, img_src):\n",
    "        img_info = self.image_to_info(img_src)\n",
    "        image_caption = img_info['caption']\n",
    "        dense_caption = img_info['dense_caption']\n",
    "        region_semantic = img_info['region_semantic']\n",
    "        width, height = img_info['shape']\n",
    "        generated_text = self.gpt_model.paragraph_summary_with_gpt(image_caption, dense_caption, region_semantic, width, height)\n",
    "        return generated_text\n",
    "    \n",
    "    # Predict the img_info for a list of image, and save the result in 'dest'\n",
    "    def forward_img_list(self, img_list, dest='result.pkl', img_root=None):\n",
    "        print('\\033[1;34m' + f\"Will process {len(img_list)} images. \".center(50, '-') + '\\033[0m')\n",
    "        info_list = []\n",
    "        for im in tqdm(img_list):\n",
    "            img_path = osp.join(img_root, im) if img_root else im\n",
    "            info = self.image_to_info(img_path)\n",
    "            info['file_name'] = im\n",
    "            info_list.append(info)\n",
    "        dump(info_list, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf7741d0-b9b6-45c9-9ba9-d231071c72c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(captioner_base_model='blip2', dense_caption=True, dense_caption_device='cuda:3', device='cuda:3', gpt_version='gpt-3.5-turbo', image_caption=True, image_caption_device='cuda:3', region_classify_model='edit_anything', sam_arch='vit_h', semantic_segment=True, semantic_segment_device='cuda:3', verbose=False)\n",
      "\u001b[1;34m----Welcome to the Image2Paragraph toolbox...-----\u001b[0m\n",
      "\u001b[1;33m--------------Initializing models...--------------\u001b[0m\n",
      "\u001b[1;31m------This is time-consuming, please wait...------\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initalize edit anything model\n",
      "\u001b[1;32m----------Model initialization finished!----------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = ImageTextTransformation(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322e498-ba1f-4d3b-8893-d86ece66cc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "im2para",
   "language": "python",
   "name": "im2para"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
